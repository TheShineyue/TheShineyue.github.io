
# üìù Publications 

## üó°Ô∏èüõ°Ô∏è Jailbreak Attacks and Defenses

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025</div><img src='images/hsr.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models** \\ **Yue Li**<sup>\*</sup>, Xin Yi<sup>\*</sup>, Dongsheng Shi, Gerard de Melo, Xiaoling Wang and Linlin Wang<sup>‚Ä†</sup>.

 [**Arxiv**](https://arxiv.org/abs/2505.16104) | [**Project**](https://github.com/TheShineyue/HSR) | [**ACL Anthology**](https://aclanthology.org/2025.findings-acl.394/)
  - The current pruning methods will lead to a significant degradation of the model's safety at a higher sparsity.
  - The HSR (**H**ierarchical **S**afety **R**ealignment) method we proposed can achieve safety realignment for the pruned model by restoring only a very small number of neurons. HSR is effective for both LLM and LVLM.
</div>
</div>

- `ESWA 2026` [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639), Xin Yi, **Yue Li**, Dongsheng Shi, Linlin Wang<sup>‚Ä†</sup>, Xiaoling Wang and Liang He.

## üìÑüîç Watermarks and Fingerprints

- `Preprint` [From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models](https://arxiv.org/abs/2509.03122), **Yue Li**<sup>\*</sup>, Xin Yi<sup>\*</sup>, Dongsheng Shi, Yongyi Cui, Gerard de Melo and Linlin Wang<sup>‚Ä†</sup>.

- `KBS 2025` [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480), Xin Yi, **Yue Li**, Shunfan Zheng, Linlin Wang<sup>‚Ä†</sup>, Xiaoling Wang and Liang He.

