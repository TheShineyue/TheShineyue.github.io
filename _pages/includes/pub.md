
# ğŸ“ Publications 

## ğŸ—¡ï¸ğŸ›¡ï¸ Jailbreak Attacks and Defenses

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025</div><img src='images/hsr.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104) \\ **Yue Li**<sup>\*</sup>, Xin Yi<sup>\*</sup>, Dongsheng Shi, Gerard de Melo, Xiaoling Wang, Linlin Wang<sup>â€ </sup>.

[**Project**](https://github.com/TheShineyue/HSR) 
  - The current pruning methods will lead to a significant degradation of the model's safety at a higher sparsity.
  - The HSR (**H**ierarchical **S**afety **R**ealignment) method we proposed can achieve safety realignment for the pruned model by restoring only a very small number of neurons. HSR is effective for both LLM and LVLM.
</div>
</div>

- `ESWA 2025` [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639), Xin Yi, **Yue Li**, Linlin Wang<sup>â€ </sup>, Xiaoling Wang and Liang He.

## ğŸ“„ğŸ” Watermarks and Fingerprints

- `Preprint` [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480), Xin Yi, **Yue Li**, Shunfan Zheng, Linlin Wang<sup>â€ </sup>, Xiaoling Wang and Liang He.
