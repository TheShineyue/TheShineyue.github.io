
# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025</div><img src='images/hsr.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://openreview.net/forum?id=) \\ 
**Yue Li**<sup>\*</sup>, Xin Yi<sup>\*</sup>, Dongsheng Shi, Gerard de Melo, Xiaoling Wang, Linlin Wang<sup>‚Ä†</sup>.

[**Project**](https://github.com/TheShineyue/HSR) 
  - The current pruning methods will lead to a significant degradation of the model's safety at a higher sparsity.
  - The HSR (**H**ierarchical **S**afety **R**ealignment) method we proposed can achieve safety realalignment for the pruned model by restoring only a very small number of neurons.
  - HSR is effective for both LLM and LVLM.
</div>
</div>

- `Preprint` [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639), Xin Yi, **Yue Li**, Linlin Wang<sup>‚Ä†</sup>, Xiaoling Wang and Liang He.

- `Preprint` [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480), Xin Yi, **Yue Li**, Shunfan Zheng, Linlin Wang<sup>‚Ä†</sup>, Xiaoling Wang and Liang He.
